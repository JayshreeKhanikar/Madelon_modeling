{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Madelon dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals in this project:\n",
    "\n",
    "With the Madelon dataset, my goal is to create a model pipeline that can reduce the dimensionality of the dataset by perform feature selection and also accurately predict the 'target' label. This dataset is synthetic by nature and is a 2-class classifiaction problem. The features are conitnuous and are not labelled which makes it hard to interpret the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "\n",
    "I am using a Madelon dataset generated by Joshua Cook which has around 200,000 instances and 1000 features. I used SQL connection through Pandas to get access to his postgres database and read the data into csv file to store. I read 20000 instances of the database at a time to read into csv file as I was unable to read the entire dataset at once due to the limitation of the T2 micro instance in AWS. \n",
    "I am using 2 samples of 20000 instances of the entire dataset to run my model pipeline \n",
    "\n",
    "## Benchmark: \n",
    "\n",
    "The target for this dataset are binary classes (1, -1), hence I fit naive LogisticRegression, KNeighborClassifier, DecisionTreeClassifier and Support Vector Classifier to get benchmark score on the the datasets. As there is no prior information about the data features and the relationships between them I am using model like LogisticRegression and SVC for linear data and KNeighbors and DecisionTreeClassifiers for nonlinear data. \n",
    "\n",
    "I used 1% of the data (2000 instances) to perform benchmarking.\n",
    "\n",
    "### Performance Metric:\n",
    "I chose to use area under the curve for Precision-Recall curve to measure the classifier's performance, as I have no real indication of what the features mean. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I separated the target column from rest of the features of dataset. \n",
    "## Feature Selection:\n",
    "\n",
    "I used 4 different techniques to search for features that are imformative and not redundant. \n",
    "\n",
    "1) I used an iterative method which checks each feature of the dataset against rest of the dataset and computed the R2 score to determine if that particular feature can predict the dataset. I use DecisionTreeClassifier as an estimator for this method. In this method I collect the R2 score for each feature, and pick top features with highest R2 score. \n",
    "\n",
    "2) I used covariance matrix to measure the covariance between each features and I selected the top features with highest covariance.\n",
    "\n",
    "3) I used SelectKBest and set 100 features to select. \n",
    "\n",
    "4) I used RandomForestClassifier inside the pipeline to Gridsearch to get feature importances. \n",
    "For all these methods I sorted the features scores or importance plotted a bar graph to visually inspect features that are informative. \n",
    "\n",
    "I came up with a list of 20 features which appeared in at least 2 of the above mentioned methods. I justify that these are the most important features in the dataset as they are selected by manyof these methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on the 20 features selected:\n",
    "\n",
    "In order the understand the distribution of the features, I did statistical analysis on the skew of data. It is clear from the histograms that the each features has bell curve distribution without any skew centered around the mean. So it is not necessary to deskew the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch classifier for tuning hyperparameters\n",
    "\n",
    "I used 20000 instances with 20 selected features of the dataset to run my models and gridsearch for the hyperparameters. I split the dat into train_test_split with test_size of 0.3.\n",
    "\n",
    "I created separate pipelines with StandardScaler, PCA and 4 estimators to tune parameters and compared the PR-AUC scores. \n",
    "\n",
    "1) LogisticRegression: I tuned penalty (l1,l2) and C with range of [0.001, 0.01, 0.1, 1, 10, 100] and trained the data with best parameter (C =100, penalty = l1)\n",
    "\n",
    "2) KNeighborsClassifier: I tuned n_neighbors in the range of [5,7,9,11,13,15,17,19,21,23,25] and trained the data with best parameter (n_neighbors = 9)\n",
    "\n",
    "3) DecisionTreeClassifer: I tuned max_depth parameter in the range of [5, 10, 15, 20, 25, 30, 35, 40] and trained the data with best parameter (max_depth = 25) \n",
    "\n",
    "4) Support Vector Classifier: I tuned C (regularization strength) [0.001, 0.01, 0.1, 1, 10] and trained the data with best parameters (C = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n",
    "A) 20 Features selected by the 4 statistical methods\n",
    "[257, 269, 308, 315, 336, 341, 395, 504, 526, 639, 681, 701, 724, 736, 769, 808, 829, 867, 920, 956]\n",
    "\n",
    "Iterative method of comparing the R2 score had clearly selected all 20 top features as rest of the 980 features R2 scores were really low. Covariance matrix selected 18 of the features with high covariance. SelectKBest with k=100 performed worst as it selected only 12-13 features whose F-scores where comparable. RandomForestClassifier selected 16-17 of the features whose importance were comparable. \n",
    "\n",
    "B) After selecting the 20 features from the dataset, I once again score the 4 classifiers to observe improvement in PR_auc score. Test score for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
